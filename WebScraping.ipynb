{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Intro to \"web scraping\"\n",
    "\n",
    "A primer to get us all on the same _page_.\n",
    "\n",
    "## What is scraping?\n",
    "\n",
    "Automating the steps of **gathering** information published on the internet and **processing** it into a convenient format for analysis.\n",
    "\n",
    " - **Gathering**: Could be as simple as downloading the results of running a query on an API, or as complicated as simulating human interaction with a web form to \"type\" search parameters \"click\" the search button and save the results.\n",
    " - **Processing**: Could be as simple as converting JSON to CSV, or as complicated as using CV to extract text from images and NLP to convert that text into tabular data.\n",
    "\n",
    "## Why do we scrape?\n",
    "\n",
    "We scrape because we have questions, but the dataset that could help answer these questions is missing.\n",
    "\n",
    "Sometimes the dataset is intentionally missing.\n",
    "Courts, for example, have databases full of tantalizing information for researchers and journalists, but they are notoriously stubborn about sharing it.\n",
    "But they are often obligated to publish some amount of information on the internet, at least for a short time.\n",
    "We can scrape the court websites to build our dataset.\n",
    "This can help uncover, for example, the major players in consumer debt collection lawsuits and how prevalent they are.\n",
    " \n",
    "Sometimes the data are piecemeal.\n",
    "Often we need to collect data from multiple sources to find a more compelling story.\n",
    "In the court data example, consumer debt is a _national_ issue and there simply doesn't exist a one single national database of consumer debt lawsuits.\n",
    "So we have to scrape one together ourselves.\n",
    "\n",
    "Sometimes the data just aren't formatted well.\n",
    "This is less true today, but in the past many websites served static HTML, possibly without a database at all backing them up.\n",
    "In order to use that data you'd need to download and format it.\n",
    "\n",
    "## Alternatives to scraping\n",
    "\n",
    "Ideally, we **don't** scrape.\n",
    "\n",
    "Scraping can be tedious and time-consuming.\n",
    "Many websites use tactics to block programmatic access.\n",
    "It's good to try other means of getting your underlying data first:\n",
    "\n",
    " - Check if there's a \"download\" button. (In 2022, this has become much more common than in the past.)\n",
    " - Ask for access. For non-sensitive / non-antagonistic projects the owners might be happy to provide you an export of their data. They likely have more and better data than you realized just by looking at the site.\n",
    " - File a public records request. This is a class unto itself, and it may not pan out the way you hope, but it's worth a shot going through the sanctioned channels.\n",
    " \n",
    "Think of scraping as a scrappy, **last resort** method of getting your dataset, when nothing else works.\n",
    "\n",
    "## Is scraping legal?\n",
    "\n",
    "[Yes!](https://techcrunch.com/2022/04/18/web-scraping-legal-court/)\n",
    "\n",
    "But! many websites actively discourage you, largely for three reasons:\n",
    "\n",
    "1. Data can be sensitive. Keep this in mind whenever _you_ are scraping data, say from a court. You will have names and details about the court cases -- and just because this is technically \"public\" information doesn't mean it's widely known, and it can negatively impact individuals to be included in a dataset you build. Be mindful of and careful with personal information.\n",
    "2. The web isn't free. Web servers cost money and have finite resources. With human users, web serves are cheap and have ample resources. But even a simple scraping bot is capable of making _dozens_ of page requests _per second_. This spike in traffic can make the website unavailable for human users and can also balloon the cost of running the server.\n",
    "3. Data is gold. The business model of the richest companies in the world is to sell ~your~ their data.\n",
    "\n",
    "Keep reasons (1) and (2) in mind as you scrape websites. Store your data ethically, censor and redact it where appropriate, and place a rate-limit on your scraping bot to keep the website available for others.\n",
    "\n",
    "Do what you want with reason (3).\n",
    "\n",
    "### What does \"actively discourage\" mean?\n",
    "\n",
    "Often, some form of CAPTCHA.\n",
    "Your IP is also logged and subject to banning.\n",
    "There are ways around these techniques, but we will not cover them here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Scoping the project\n",
    "\n",
    "The first things to identify _before_ you start scraping are:\n",
    "1. What's your question?\n",
    "2. What data do you need to answer your question?\n",
    "3. Where can you get this data?\n",
    "\n",
    "For today's project, we're going to look at housing and evictions in San Francisco.\n",
    "\n",
    "San Francisco is one of the most [expensive](https://www.dbresearch.com/PROD/RPS_EN-PROD/PROD0000000000494405.pdf?undefined&realload=IfbV/lNZWzJUGuuE7hHWFmrrGl3IRC7Wm1wixcHx0ltY2AZL6G3khovJo4kh22HV) rental markets in the world.\n",
    "People often point to the basics of supply and demand to explain this: there aren't enough rental units.\n",
    "While there are certainly multiple reasons why rental supply is limited, one factor that seems awfully dubious is known as the [_Ellis Act_](https://sftu.org/ellis/).\n",
    "\n",
    "While California (and San Francisco in particular) typically has strong protections for tenants, under Ellis, landlords can unconditionally evict tenants if they are taking the building out of the rental market.\n",
    "The law was intended to give landlords an out if they wanted to, say, have a family and stop renting out their downstairs unit.\n",
    "\n",
    "However, there is a suspicion that real estate speculators have started abusing Ellis to take rental units off the market and flip them as houses.\n",
    "\n",
    "\n",
    "### Question\n",
    "\n",
    "How has the Ellis Act been used in San Francisco?\n",
    "\n",
    "_Big question! We will only be able to begin to address this in this workshop!_\n",
    "\n",
    "\n",
    "### What data do we want?\n",
    "\n",
    "Today, we will start by gathering only one statistic:\n",
    "\n",
    " - How many evictions have been filed under the Ellis Act\n",
    " \n",
    "To follow up on this work, we'd want to see also:\n",
    "\n",
    " - Who has been filing the Ellis Act\n",
    " - What are the addresses of units that have been Ellised\n",
    " - What's the transaction timeline of the houses in question\n",
    " \n",
    "### Where can we get data?\n",
    "\n",
    "Today we will look at:\n",
    "\n",
    " - The [SF Rent Board](https://sfrb.org/monthly-statistics) publishes monthly statistics, including how many units have been Ellised\n",
    " \n",
    "In the future we might also want to see data from:\n",
    "\n",
    " - The [SF Assessor/Recorder](https://sfplanninggis.org/pim/?pub=true), who track information about property ownership for tax purposes, including who owns the property and what the property is used for.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Checking out the site\n",
    "\n",
    "So today we will get data the SF Rent Board.\n",
    "\n",
    "Let's check out their statistics site: https://sfrb.org/monthly-statistics\n",
    "\n",
    "Cool, about 20 years of data!!\n",
    "Let's check out what format it's in. Click on one of the links, like: https://sfrb.org/sites/default/files/Workload%20Stats%20April%202022.pdf\n",
    "\n",
    "_Gahhh, a PDF!_\n",
    "\n",
    "Well, the good news is this form contains an _Ellis_ field, and it looks like it's been filled in every month since ... 2003?\n",
    "Looks like from 2000-02 they actually published this in HTML! ðŸ™ƒ\n",
    "\n",
    "### Sometimes there are easier ways\n",
    "\n",
    "For this site, we're going to have to extract data from PDFs.\n",
    "No way around it.\n",
    "\n",
    "For newer sites, it's worth checking the \"Network\" tab in your browser - there may be a query API \"under the hood\" that you can interact with directly, without having to process HTML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Setting up your environment\n",
    "\n",
    "Python seems like a good choice here.\n",
    "For highly dynamic websites in which you have to simulate \"human\" behavior, it's often easier to write scrapers in JavaScript.\n",
    "But for this project, the Python code will end up being simpler and cleaner.\n",
    "\n",
    "We'll install a few packages to help:\n",
    "\n",
    "### [`requests`](https://requests.readthedocs.io/en/latest/)\n",
    "\n",
    "Python includes built-in libraries to make HTTP requests, but they are very _low-level_, meaning they have lots of options and tend to be tedious to use. Most people use a library called `requests` to simplify this process.\n",
    "\n",
    "### [`beautifulsoup4`](https://www.crummy.com/software/BeautifulSoup/)\n",
    "\n",
    "There are many ways to process HTML. BeautifulSoup is a library developed specifically for scraping web-pages. It is able to parse messy (and potentially invalid) HTML and provides flexible ways to query for information in the parsed document.\n",
    "\n",
    "### [`PyPDF2`](https://pypdf2.readthedocs.io/en/latest/)\n",
    "\n",
    "Python does not include a built-in way to work with PDF files, so we use this 3rd-party library to help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install requests beautifulsoup4 PyPDF2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Scraping the index\n",
    "\n",
    "Time to start scraping!\n",
    "\n",
    "Our first goal is to get a list of all the documents that contain the information we want.\n",
    "Very commonly when scraping websites, we run into this kind of \"index\" page that contains links to the actual content.\n",
    "On the SFRB site, this actual content is (mostly) in the form of PDFs.\n",
    "\n",
    "### Index scraping goals:\n",
    "1. Fetch the HTML of the \"index\" page\n",
    "2. Find links to every PDF containing housing data\n",
    "3. Download PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Fetch the index page\n",
    "\n",
    "Use `requests` to download the HTML of the index page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "URL = \"https://sfrb.org/monthly-statistics\"\n",
    "response = requests.get(URL)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was easy! but ... what a **_soup!_**\n",
    "\n",
    "## 3.2. Find PDF links\n",
    "\n",
    "All that HTML is going to be a pain to sift through.\n",
    "This is where BeautifulSoup comes in handy.\n",
    "\n",
    "Let's take a look at some tools BeautifulSoup gives us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the `BeautifulSoup` class.\n",
    "# Note that `beautifulsoup4` installs itself as `bs4` so you don't have to type as much.\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Creating a `BeautifulSoup` object will parse the HTML.\n",
    "soup = BeautifulSoup(response.text)\n",
    "\n",
    "# Now `soup` has a few methods for interacting with the parsed HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Query for all the `h1` elements:\\n\")\n",
    "print(soup.find_all('h1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Query for all the elements with a `block-title` class:\\n\")\n",
    "print(soup.select('.block-title'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extract the text content of all the `.block-title` elements:\\n\")\n",
    "for el in soup.select('.block-title'):\n",
    "    print(el.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`BeautifulSoup` has *lots* more features you may need at some point. For now, let's just find the PDF links.\n",
    "\n",
    "Using the \"inspect\" tool in your browser, you want to figure out: what makes the links I'm interested in _unique_, compared with any other link on the page?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Way too broad!\n",
    "all_links = soup.select('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is better, but still too broad ...\n",
    "li_links = soup.select('li a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Even better, but what about those non-PDF links?\n",
    "table_links = soup.select('table li a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is actually too narrow because they used a CMS at some point!!!\n",
    "pdf_ext_links = soup.select('a[href*=pdf]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if we use the fact that all the PDF link texts indicate they're PDFs with the `(pdf)`?\n",
    "table = soup.find('table')\n",
    "pdf_links = [a for a in table.select('a') if '(pdf)' in a.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# That worked! now let's pull the full URL:\n",
    "urls = [a['href'] for a in pdf_links]\n",
    "\n",
    "# Oops, except some of those are relative links. So let's normalize it.\n",
    "base_url = 'https://sfrb.org'\n",
    "\n",
    "def fix_url(href):\n",
    "    if href.startswith('/'):\n",
    "        return base_url + href\n",
    "    return href\n",
    "\n",
    "urls = [fix_url(a['href']) for a in pdf_links]\n",
    "\n",
    "# Great!! But let's make sure we have a good name for the files, too.\n",
    "urls = {a.text : fix_url(a['href']) for a in pdf_links}\n",
    "\n",
    "# Well... that's not really a *good* name. Let's clean it up a bit more.\n",
    "import re\n",
    "def fix_name(text):\n",
    "    # Split the text after the four digit year, since the end is just junk.\n",
    "    parts = re.split(r'(\\d{4})', text)\n",
    "    # Recombine the month and the year parts from the split result.\n",
    "    return parts[0] + parts[1] + '.pdf'\n",
    "\n",
    "urls = {fix_name(a.text): fix_url(a['href']) for a in pdf_links}\n",
    "\n",
    "print(f\"Found {len(urls)} PDFs to download!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Download the PDFs\n",
    "\n",
    "Now we just need to loop through the PDF links and save them.\n",
    "\n",
    "Remember, we are going to make over 200 requests to the server to download documents.\n",
    "This is a red flag to see this burst of activity from one IP address.\n",
    "It's very possible they will ban your IP automatically.\n",
    "It's also possible for you to degrade performance for other users.\n",
    "\n",
    "You can mitigate this by spacing out your requests a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "# Iterate over every URL we have.\n",
    "for name, url in urls.items():\n",
    "    # Get the output path for the data. This will use the `data/` directory, so that\n",
    "    # we don't clutter the root directory.\n",
    "    file_name = os.path.join('raw', name)\n",
    "    # Skip downloading files we already have (in case of running this multiple times!)\n",
    "    if os.path.exists(file_name):\n",
    "        print(f\"Already have {name}!\")\n",
    "        continue\n",
    "        \n",
    "    print(f\"Downloading {name} ...\")\n",
    "    # Create a local file with the name we generated.\n",
    "    #\n",
    "    # Note that we open the file for *writing* in *binary* mode, since PDFs\n",
    "    # are binary (not textual) data.\n",
    "    with open(file_name, 'wb') as fh:\n",
    "        # Request the URL from the server\n",
    "        result = requests.get(url)\n",
    "        # Save the raw response in the local file\n",
    "        fh.write(result.content)\n",
    "        # Clean up the response object\n",
    "        result.close()\n",
    "\n",
    "    # Sleep for a little bit to space out the requests.\n",
    "    time.sleep(0.25)\n",
    "    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Parsing the PDFs\n",
    "\n",
    "Now that we have the PDFs, it's time to process them, similarly to how we processed the index page.\n",
    "\n",
    "`BeautifulSoup` worked great for the HTML, but it won't work for PDFs.\n",
    "Instead, we will use a library called `PyPDF2`.\n",
    "\n",
    "The goals for this step are:\n",
    "1. Extract text from the PDFs\n",
    "2. Parse Ellis filings from the extracted text\n",
    "3. Create a CSV file with Ellis info by date\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Extracting text from PDFs\n",
    "\n",
    "Let's look at what the PyPDF2 library gives us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfFileReader\n",
    "\n",
    "reader = PdfFileReader(os.path.join('raw', 'April 2022.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The PDF contains some metadata about its creation.\n",
    "print(\"Metadata:\")\n",
    "for key, value in reader.documentInfo.items():\n",
    "    print(f\"  {key}: {value.get_object()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It also can tell us how many pages it has:\n",
    "print(f\"Pages: {reader.getNumPages()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There's a lot more information that can be helpful, but let's\n",
    "# just look directly at the text of that single page in the document:\n",
    "text = reader.getPage(0).extract_text()\n",
    "print(\"Text:\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More soup!\n",
    "\n",
    "## 4.2. Extracting Ellis filings\n",
    "\n",
    "The text is kind of messy, but hey, we can see the word \"Ellis\" and some numbers.\n",
    "That's very promising!\n",
    "\n",
    "We can write a regular expression to extract the two numbers after we see the word \"Ellis.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# This pattern looks for the string \"Ellis)\" then finds the first two\n",
    "# numbers after that.\n",
    "#\n",
    "# Notes:\n",
    "#  1) We make sure to account for the left parenthesis which we see in the document.\n",
    "#     If you are working with OCRed documents, you need to be careful and creative about\n",
    "#     these types of symbols! Our document was created digitally, so we don't have that issue.\n",
    "#  2) We use a variable number of whitespace \\s characters with the + sign. This literally\n",
    "#     means \"one or more,\" because it isn't clear how many spaces are expected to be there,\n",
    "#     and it may change between documents.\n",
    "#  3) We use capturing groups with the parentheses to enclose the \"number\" patterns: (\\d+).\n",
    "#     This will let us extract the numbers with the `.groups()` method on the match object.\n",
    "match = re.search(r'Ellis\\)\\s+(\\d+)\\s+(\\d+)', text)\n",
    "print(match.groups())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this to what we see in the \"April 2022.pdf\" document.\n",
    "That looks correct!\n",
    "\n",
    "**Scaling up**\n",
    "\n",
    "Now lets loop through all the documents and run this pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the results of our extraction.\n",
    "ellis_data = {}\n",
    "\n",
    "# Compiling the pattern in advance is good practice, but not necessary.\n",
    "ellis_pattern = re.compile(r'Ellis\\)\\s+(\\d+)\\s+(\\d+)')\n",
    "\n",
    "for name in urls.keys():\n",
    "    print(f\"Parsing {name} ...\")\n",
    "    reader = PdfFileReader(os.path.join('raw', name))\n",
    "    text = reader.getPage(0).extract_text()\n",
    "    match = ellis_pattern.search(text)\n",
    "    ellis_data[name.strip('.pdf')] = match.groups()\n",
    "    \n",
    "print(ellis_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's looking a lot like the dataset we were seeking!!\n",
    "\n",
    "## 4.3. Saving results\n",
    "\n",
    "People usually save data as CSV files.\n",
    "These are simple and portable.\n",
    "You can share a CSV and immediately open it in Excel, Google Sheets, Apple Numbers, Python, R, or any old text editor.\n",
    "\n",
    "**hot take**\n",
    "\n",
    "The downside of CSV is that it does not store any explicit _type_ information.\n",
    "So, if your data contains numbers, dates, null values, and so on, you (and anyone you give the data to) will have to figure that out themselves when they open the file.\n",
    "\n",
    "I personally feel it's more useful to use a structured data format such as parquet, sqlite, or even JSON.\n",
    "\n",
    "But for now, we will just use CSV.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Our data is currently a Dict that looks like {date: [petitions, units]}.\n",
    "# To write a CSV, we need a List of rows.\n",
    "#\n",
    "# Each row should contain the following columns:\n",
    "cols = ['year', 'month', 'petitions', 'units']\n",
    "\n",
    "# Note that we are changing `date` to two columns `(year, month)`. This will\n",
    "# make the file a bit more flexible, since we could aggregate it by year,\n",
    "# or look at year-over-year trends.\n",
    "rows = []\n",
    "for date, data in ellis_data.items():\n",
    "    month, year = date.split()\n",
    "    # Note that all of the values are strings when we create the row.\n",
    "    # Remember that CSV doesn't store any type information, so a number is\n",
    "    # stored exactly the same as a string. The only point of trying to convert\n",
    "    # here would be to help validate the data.\n",
    "    #\n",
    "    # If we were using a different file type, we should convert the year,\n",
    "    # petitions, and units columns to integer types.\n",
    "    rows.append({\n",
    "        'year': year,\n",
    "        'month': month,\n",
    "        'petitions': data[0],\n",
    "        'units': data[1],\n",
    "    })\n",
    "\n",
    "# Now create a CSV file and write the data.\n",
    "with open(os.path.join('data', 'ellis.csv'), 'w') as fh:\n",
    "    writer = csv.DictWriter(fh, cols)\n",
    "    # First write a \"header\" row so anyone using the file knows what the\n",
    "    # columns mean.\n",
    "    writer.writeheader()\n",
    "    # Now write each row\n",
    "    for row in rows:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Now open `data/ellis.csv` and behold: tabular data!!_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Working with the data\n",
    "\n",
    "The scraping project is officially **done**, but we might as well take a look at what we found!\n",
    "We have all sorts of power now that we have this table.\n",
    "Let's just try to do a few basic things:\n",
    "\n",
    "1. Read data from the CSV\n",
    "2. Tally how many units have been taken off the market via the Ellis Act in our data\n",
    "3. Make a chart of Ellis petitions over time\n",
    "\n",
    "## 5.1. Reading the CSV\n",
    "\n",
    "Reading the CSV is even easier than writing it, though, again we need to be careful about our data types.\n",
    "We know that we're working with integers and we don't have any missing values, so we'll convert the columns here.\n",
    "\n",
    "In a larger analysis project you would want to use a fully-featured library like `pandas`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "data = []\n",
    "with open(os.path.join('data', 'ellis.csv'), 'r') as fh:\n",
    "    csv_reader = csv.DictReader(fh)\n",
    "    for line in csv_reader:\n",
    "        # Convert the numeric columns to integers.\n",
    "        # This would fail if we were missing data, or the columns were invalid,\n",
    "        # So be careful about that!\n",
    "        line['year'] = int(line['year'])\n",
    "        line['petitions'] = int(line['petitions'])\n",
    "        line['units'] = int(line['units'])\n",
    "        data.append(line)\n",
    "\n",
    "print(\"Loaded data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Math!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many total Ellis petitions?\n",
    "total_petitions = sum(d['petitions'] for d in data)\n",
    "print(\"Total Ellis petitions:\", total_petitions)\n",
    "\n",
    "# How many total units have been Ellised?\n",
    "total_units = sum(d['units'] for d in data)\n",
    "print(\"Total Ellised units:\", total_units)\n",
    "\n",
    "# Average units per petition?\n",
    "print(\"Average units per petition\", total_units / total_petitions)\n",
    "\n",
    "# How many units were Ellised per year?\n",
    "petitions_by_year = {}\n",
    "for d in data:\n",
    "    if d['year'] not in petitions_by_year:\n",
    "        petitions_by_year[d['year']] = 0\n",
    "    petitions_by_year[d['year']] += d['petitions']\n",
    "    \n",
    "print(\"Petitions by year:\")\n",
    "ordered_years = sorted(petitions_by_year.keys())\n",
    "for year in ordered_years:\n",
    "    print(f\"  {year}: {petitions_by_year[year]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Visualization!\n",
    "\n",
    "The by-year table is interesting, but it would be easier to make sense of in a chart.\n",
    "\n",
    "We'll use a charting library called `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# We can make a really quick plot by year, since we already have the data.\n",
    "ordered_petitions = [petitions_by_year[year] for year in ordered_years]\n",
    "plt.plot(ordered_years, ordered_petitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make an actual time series, we'll need to convert (year, month) to a date type.\n",
    "from datetime import datetime\n",
    "\n",
    "time_series = []\n",
    "petitions_series = []\n",
    "units_series = []\n",
    "for d in data:\n",
    "    # `strptime` parses dates in the given format.\n",
    "    # %B means the full month name, like \"January\"\n",
    "    # %Y means the 4-digit year, like \"2002.\"\n",
    "    date = datetime.strptime(f\"{d['month']} {d['year']}\", \"%B %Y\")\n",
    "    # Store all the values in separate series, for the plotting library\n",
    "    time_series.append(date)\n",
    "    petitions_series.append(d['petitions'])\n",
    "    units_series.append(d['units'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the petitions by year\n",
    "plt.plot(time_series, petitions_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the units Ellised by year\n",
    "plt.plot(time_series, units_series)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
